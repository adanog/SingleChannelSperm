{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64874885-d741-4267-aa9d-bb40cb567099",
   "metadata": {},
   "source": [
    "# Time Series Analysis of Putative Single Channel Transients\n",
    "Analysis of the integrated signal of a ROI of a line scan experiment. \n",
    "\n",
    "The ROI was selected from the region of maxima variance (Var) change dVar/dr in space (r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714bd13-caf7-453a-878f-e16282ffd243",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38e73c6-fc79-4862-a60a-acb33f48a7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing necessary libraries...\")\n",
    "!pip install ome-zarr > /dev/null 2>&1\n",
    "print(\"Libraries installed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a7331-7cc9-4c5e-8689-6739649eaddc",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f36178-5fd6-437f-ad9c-ef185b7334bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zarr\n",
    "from ome_zarr.io import parse_url\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import tifffile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91382ec4-c51a-4687-afbe-6d62a7281ef3",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50928d1b-118e-4152-9925-34cafec18f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roi_at_high_variance(F5N_image, metadata, apply_blur = False):\n",
    "    \"\"\"\n",
    "    Creates a roi at the region of higher change of temporal variance\n",
    "    \n",
    "    Parameters:\n",
    "        F5N_image (numpy.ndarray): The input 2D fluorescence image.\n",
    "        metadata (dict): Metadata containing keys like 'pixel_size', 'time_domain', and 'time_per_line'.\n",
    "        acrosome_fwhm (float): Full-width half-maximum of the acrosome in nanometers. Default is 380.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results containing the adjusted window start, width, and non-stationary noise analysis.\n",
    "    \"\"\"\n",
    "    # Apply Gaussian blur if sigma_blur is provided\n",
    "    if apply_blur is not False:\n",
    "        acrosome_fwhm = 260 # nm (modelled using script 06_fluorescence_disk_simulation.ipynb)\n",
    "        s = (acrosome_fwhm/2.35)/1000 # in microns\n",
    "        spatial_blur = s/(metadata['pixel_size']*1) # blur one third the width of s\n",
    "        # Set sigma only for the axis corresponding to rows (axis 0 in 2D array)\n",
    "        sigma = [1, spatial_blur]  # Blur rows only, keep columns untouched\n",
    "        F5N_image = gaussian_filter(F5N_image, sigma=sigma)\n",
    "    \n",
    "    # Compute variance and its derivative\n",
    "    variance_values = np.var(F5N_image, axis=0)\n",
    "    derivative_variance = np.gradient(variance_values)\n",
    "\n",
    "    # Find the start of the window\n",
    "    w_start = np.argmax(derivative_variance)\n",
    "\n",
    "    # Compute the window width (convert to pixels)\n",
    "    w_width = math.floor(0.8 / metadata['pixel_size'])\n",
    "\n",
    "    # Adjust the start index if the window goes out of bounds\n",
    "    if w_start + w_width > F5N_image.shape[1]:\n",
    "        # Shift the window to the left to fit within bounds\n",
    "        w_start = F5N_image.shape[1] - w_width\n",
    "    if w_start < 0:\n",
    "        # Shift the window to the right to fit within bounds\n",
    "        w_start = 0\n",
    "\n",
    "    # Ensure w_width does not exceed the remaining matrix domain\n",
    "    w_width = min(w_width, F5N_image.shape[1] - w_start)\n",
    "\n",
    "    image = F5N_image[:, w_start:(w_start+w_width)]\n",
    "\n",
    "    # Display the first 32 rows and the selected window of columns\n",
    "    plt.imshow(image[:150,:], cmap=\"gray\")\n",
    "    plt.title(\"First 150 lines of high var carpet\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"The adjusted index of the window start is: {w_start}\")\n",
    "    print(f\"The adjusted window width in pixels is: {w_width}\")\n",
    "\n",
    "    # Return relevant results as a dictionary\n",
    "    return image\n",
    "\n",
    "\n",
    "def process_ome_zarr_image(ome_zarr_path):\n",
    "    \"\"\"\n",
    "    Processes an OME-Zarr image by extracting a specific slice, applying Gaussian blur, and retrieving metadata.\n",
    "\n",
    "    Parameters:\n",
    "    - ome_zarr_path (str): Path to the OME-Zarr file.\n",
    "\n",
    "    Returns:\n",
    "    - F5N_image_64x64 (ndarray): Processed 64x64 pixel image slice.\n",
    "    - metadata (dict): Dictionary containing extracted metadata:\n",
    "        - 'time_per_frame' (float or str): Total time per frame in milliseconds; returns a message if not found.\n",
    "        - 'time_per_line' (float or str): Time per line in milliseconds; returns a message if not found.\n",
    "        - 'pixel_size' (float or str): Pixel size; returns a message if not found.\n",
    "        - 'time_domain' (ndarray or None): Time domain in seconds; None if time per frame is not found.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the OME-Zarr file\n",
    "    root = zarr.open(ome_zarr_path, mode=\"r\")\n",
    "   # Access the Fluo-5N image slice (T=0, C=3, Z=0)\n",
    "    img = root[\"image_data\"][0, 0, 0, :, :]  # Adjust indices as needed\n",
    "\n",
    "    # Retrieve metadata\n",
    "    image_metadata = root.attrs.get(\"image_metadata\", {})\n",
    "\n",
    "    # Initialize metadata dictionary\n",
    "    metadata = {}\n",
    "\n",
    "    # Extract time per frame\n",
    "    time_per_frame = image_metadata.get('Axis 4 Parameters Common', {}).get('EndPosition', None)\n",
    "    if time_per_frame is None:\n",
    "        metadata['time_per_frame'] = 'Time Per Frame not found'\n",
    "        metadata['time_per_line'] = 'Time Per Line not found'\n",
    "        metadata['time_domain'] = None\n",
    "    else:\n",
    "        # Calculate time per line\n",
    "        time_per_line = time_per_frame / img.shape[0]\n",
    "        # Convert time to seconds\n",
    "        time_domain = np.arange(img.shape[0]) * (time_per_line / 1000)\n",
    "        metadata['time_per_frame'] = time_per_frame\n",
    "        metadata['time_per_line'] = time_per_line\n",
    "        metadata['time_domain'] = time_domain\n",
    "\n",
    "    # Extract pixel size\n",
    "    pixel_size = image_metadata.get('Reference Image Parameter', {}).get('WidthConvertValue', 'Pixel Size not found')\n",
    "    metadata['pixel_size'] = pixel_size\n",
    "    metadata['pixels_per_line'] =  img.shape[1]\n",
    "    return img, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe9f93-6ec9-4457-8542-e027f67d637e",
   "metadata": {},
   "source": [
    "## Set Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d52937-35ee-4903-aace-209fb8212375",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = '/home/jovyan/LNMA/guerreroa/data/20241122'\n",
    "filename1 = '20241122_MBF1uMXT5Min_Head_5000L_PHC_4us_Zo25_128x128_R4_C2.zarr'\n",
    "ome_zarr_path1 = os.path.join(directory_path, filename1)\n",
    "\n",
    "filename2 = '20241122_MBF30uMXT5Min_Head_5000L_PHC_4us_Zo25_128x128_R4_C1.zarr'\n",
    "ome_zarr_path2 = os.path.join(directory_path, filename2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e3e29-80cd-45d5-baa7-39bc50e4674a",
   "metadata": {},
   "source": [
    "## Perform Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4718f2e6-812e-453f-a6f4-e76426aee8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1, metadata1 = process_ome_zarr_image(ome_zarr_path1)\n",
    "img1 = roi_at_high_variance(img1, metadata1, apply_blur = True)\n",
    "tifffile.imwrite(os.path.join(directory_path, 'img1.tif'), img1[:150,:])\n",
    "\n",
    "f1= img1.sum(axis=1)\n",
    "img2, metadata2 = process_ome_zarr_image(ome_zarr_path2)\n",
    "img2 = roi_at_high_variance(img2, metadata2, apply_blur = True)\n",
    "tifffile.imwrite(os.path.join(directory_path, 'img2.tif'), img2[:150,:])\n",
    "\n",
    "f2= img2.sum(axis=1)\n",
    "r = range(0, 150)\n",
    "\n",
    "plt.plot( metadata1['time_domain'][r], f1[r])\n",
    "plt.plot( metadata1['time_domain'][r], f2[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014c7f45-ee8c-49ba-9825-c34c6f1b353e",
   "metadata": {},
   "outputs": [],
   "source": [
    " metadata1['time_per_frame'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc8932-d6f0-4c84-8447-61b1e0f196dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import poisson\n",
    "\n",
    "def bin_time_series(data, acq_time, n_intervals, plot=False):\n",
    "    bin_size = len(data) // n_intervals\n",
    "    binned_counts = [np.mean(data[i * bin_size:(i + 1) * bin_size]) for i in range(n_intervals)]\n",
    "    binned_time = [acq_time * bin_size * (i + 0.5) for i in range(n_intervals)]\n",
    "    if plot:\n",
    "        plt.plot(binned_time, binned_counts, label=\"Binned Data\")\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Mean Counts\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return {\"Counts\": np.array(binned_counts), \"Time\": np.array(binned_time)}\n",
    "\n",
    "def detrend_time_series(f, acq_time, n_intervals, algorithm, degree=None, w=None, pois=False, max_val=False, plot=True):\n",
    "    # Bin the time series\n",
    "    binned = bin_time_series(f, acq_time, n_intervals, plot=False)\n",
    "    \n",
    "    if algorithm.lower() == \"exp\":\n",
    "        # Exponential model fitting\n",
    "        def exp_model(t, a0, k):\n",
    "            return a0 * np.exp(k * t)\n",
    "\n",
    "        params, _ = curve_fit(exp_model, binned['Time'], binned['Counts'])\n",
    "        a0, k = params\n",
    "        full_time = np.arange(len(f)) * acq_time\n",
    "        full_model = exp_model(full_time, a0, k)\n",
    "        h = np.exp(max(np.log(binned['Counts'])) if max_val else np.log(binned['Counts'][0]))\n",
    "        if pois:\n",
    "            full_residuals = f + poisson.rvs(np.abs(h - full_model))\n",
    "        else:\n",
    "            full_residuals = f - full_model + h\n",
    "    \n",
    "    elif algorithm.lower() == \"poly\":\n",
    "        # Polynomial fitting\n",
    "        coeffs = np.polyfit(binned['Time'], binned['Counts'], degree)\n",
    "        poly_model = np.poly1d(coeffs)\n",
    "        full_time = np.arange(len(f)) * acq_time\n",
    "        full_model = poly_model(full_time)\n",
    "        h = max(poly_model(binned['Time'])) if max_val else poly_model(binned['Time'][0])\n",
    "        if pois:\n",
    "            full_residuals = [residual + poisson.rvs(h) for residual in (f - full_model)]\n",
    "        else:\n",
    "            full_residuals = f - full_model + h\n",
    "    \n",
    "    elif algorithm.lower() == \"boxcar\":\n",
    "        if w <= 1 or w > len(f) // 2:\n",
    "            raise ValueError(\"'w' must be greater than 1 and smaller than half the length of 'f'\")\n",
    "        x = np.convolve(f, np.ones(w) / w, mode='valid')\n",
    "        h = max(x) if max_val else x[0]\n",
    "        if pois:\n",
    "            full_residuals = f[:len(x)] - x + poisson.rvs(h, size=len(x))\n",
    "        else:\n",
    "            full_residuals = f[:len(x)] - x + h\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Select a valid detrending algorithm ('exp', 'poly', or 'boxcar')\")\n",
    "    \n",
    "    # Bin the residuals for plotting\n",
    "    residual_binned = bin_time_series(full_residuals, acq_time, n_intervals, plot=False)\n",
    "    \n",
    "    # Plot results\n",
    "    # Plot results\n",
    "    if plot:\n",
    "        plt.plot(binned['Time'], binned['Counts'], label=\"Original Binned Data\", color=\"black\")\n",
    "        plt.plot(binned['Time'], exp_model(binned['Time'], a0, k), label=\"Trend Model\", color=\"red\")  # Ensure correct x-axis\n",
    "        plt.plot(residual_binned['Time'], residual_binned['Counts'], label=\"Detrended Data\", color=\"blue\")\n",
    "        plt.axhline(h, color=\"green\", linestyle=\"--\", label=\"Trend Correction Value\")\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Mean Counts\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Detrended Time Series\")\n",
    "        plt.show()\n",
    "    \n",
    "    return full_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def5d0ea-c439-4415-911c-a90b6fa72eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exponential detrending\n",
    "f1_det = detrend_time_series(\n",
    "    f=f1,\n",
    "    acq_time=metadata1['time_per_line']*1e-3,\n",
    "    n_intervals=100,\n",
    "    algorithm=\"exp\",\n",
    "    pois=False,\n",
    "    max_val=False\n",
    ")\n",
    "\n",
    "f2_det = detrend_time_series(\n",
    "    f=f2,\n",
    "    acq_time=metadata2['time_per_line']*1e-3,\n",
    "    n_intervals=100,\n",
    "    algorithm=\"exp\",\n",
    "    pois=False,\n",
    "    max_val=False\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(metadata1['time_domain'][r], f1_det[r], label='Detrended f1')\n",
    "plt.plot(metadata1['time_domain'][r], f2_det[r], label='Detrended f2')\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Mean Counts\")\n",
    "plt.title(\"Detrended Time Series\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(directory_path, 'detrended_time_series.pdf'), format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8a833a-4caa-4fbc-8110-657d27b508d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ks_2samp, mannwhitneyu\n",
    "\n",
    "# Compute histograms\n",
    "bins = np.linspace(min(min(f1_det), min(f2_det)), max(max(f1_det), max(f2_det)), 120)  # Shared bin edges\n",
    "hist1, edges1 = np.histogram(f1_det, bins=bins, density=True)\n",
    "hist2, edges2 = np.histogram(f2_det, bins=bins, density=True)\n",
    "\n",
    "# Compute the bin centers for plotting\n",
    "bin_centers = (edges1[:-1] + edges1[1:]) / 2\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(bin_centers, hist1, label=\"Time Series 1 (f1_det)\", color=\"blue\", linewidth=2)\n",
    "plt.plot(bin_centers, hist2, label=\"Time Series 2 (f2_det)\", color=\"red\", linewidth=2)\n",
    "plt.xlabel(\"F (a.u)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Probability Distribution of Calcium Channel Openings\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Statistical tests\n",
    "# Kolmogorov-Smirnov Test\n",
    "ks_stat, ks_pval = ks_2samp(f1_det, f2_det)\n",
    "print(f\"Kolmogorov-Smirnov Test: statistic={ks_stat:.4f}, p-value={ks_pval:.4e}\")\n",
    "\n",
    "# Mann-Whitney U Test\n",
    "u_stat, u_pval = mannwhitneyu(f1_det, f2_det, alternative=\"two-sided\")\n",
    "print(f\"Mann-Whitney U Test: statistic={u_stat:.4f}, p-value={u_pval:.4e}\")\n",
    "\n",
    "# Interpretation\n",
    "if ks_pval < 0.05:\n",
    "    print(\"Kolmogorov-Smirnov Test: The two distributions are significantly different.\")\n",
    "else:\n",
    "    print(\"Kolmogorov-Smirnov Test: No significant difference between the two distributions.\")\n",
    "\n",
    "if u_pval < 0.05:\n",
    "    print(\"Mann-Whitney U Test: The two distributions are significantly different.\")\n",
    "else:\n",
    "    print(\"Mann-Whitney U Test: No significant difference between the two distributions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d412c-8b8f-4d6e-aa9e-0bab21ab1771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import root_scalar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Example time series (replace these with f1_det and f2_det)\n",
    "# Replace `f1_det` and `f2_det` with your actual data\n",
    "bins = np.linspace(min(min(f1_det), min(f2_det)), max(max(f1_det), max(f2_det)), 120)  # Shared bins\n",
    "\n",
    "# Compute histograms (normalized to probability densities)\n",
    "hist1, edges1 = np.histogram(f1_det, bins=bins, density=True)\n",
    "hist2, edges2 = np.histogram(f2_det, bins=bins, density=True)\n",
    "\n",
    "# Bin centers\n",
    "bin_centers = (edges1[:-1] + edges1[1:]) / 2\n",
    "\n",
    "# Interpolate the histograms\n",
    "interp_hist1 = interp1d(bin_centers, hist1, kind='linear', fill_value=\"extrapolate\")\n",
    "interp_hist2 = interp1d(bin_centers, hist2, kind='linear', fill_value=\"extrapolate\")\n",
    "\n",
    "# Define the function to find the root (intersection)\n",
    "def intersection(x):\n",
    "    return interp_hist1(x) - interp_hist2(x)\n",
    "\n",
    "# Solve for the root (cross-point) within the range of bin_centers\n",
    "cross_point = root_scalar(intersection, bracket=[min(bin_centers), max(bin_centers)], method='brentq').root\n",
    "\n",
    "\n",
    "\n",
    "# Plot the histograms and the cross point\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.plot(bin_centers, hist1, label=\"1 µM Mb\", linewidth=2)\n",
    "plt.plot(bin_centers, hist2, label=\"30 µM Mb\", linewidth=2)\n",
    "plt.axvline(cross_point, color=\"green\", linestyle=\"--\", label=f\"Cross Point: {cross_point:.4f}\")\n",
    "plt.xlabel(\"F (a.u)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Probability Distribution\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(directory_path, 'Probability Distribution of fluorescent transients.pdf'), format='pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Print the exact cross point\n",
    "print(f\"Exact Cross Point: {cross_point:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d333f6e-78ca-4823-83a2-8e7e5859719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to compute waiting times over the threshold\n",
    "def compute_waiting_times(time_series, threshold, time_step):\n",
    "    above_threshold = time_series > threshold\n",
    "    waiting_times = []\n",
    "    crossing_indices = []\n",
    "    \n",
    "    # Iterate through the time series to track waiting times correctly\n",
    "    start_index = None\n",
    "    for i in range(len(time_series)):\n",
    "        if above_threshold[i]:\n",
    "            if start_index is None:\n",
    "                start_index = i  # Start counting when the threshold is crossed\n",
    "            crossing_indices.append(i)\n",
    "        elif start_index is not None:\n",
    "            # Compute waiting time when the signal drops below the threshold\n",
    "            waiting_times.append((i - start_index) * time_step)\n",
    "            start_index = None  # Reset for the next crossing\n",
    "    \n",
    "    return waiting_times, crossing_indices\n",
    "\n",
    "# Replace with your own data and parameters\n",
    "#r = range(0, 20)  # Define the range for your data\n",
    "threshold_f1 = cross_point  # Replace with your threshold for f1\n",
    "threshold_f2 = cross_point  # Replace with your threshold for f2\n",
    "time_step = metadata2['time_per_line']  # Time step from metadata\n",
    "\n",
    "# Compute waiting times and crossing indices for the full series\n",
    "waiting_times_f1, crossing_indices_f1_full = compute_waiting_times(f1_det, threshold_f1, time_step)\n",
    "waiting_times_f2, crossing_indices_f2_full = compute_waiting_times(f2_det, threshold_f2, time_step)\n",
    "\n",
    "# Filter crossing indices to match the range `r`\n",
    "crossing_indices_f1 = [idx for idx in crossing_indices_f1_full if idx in r]\n",
    "crossing_indices_f2 = [idx for idx in crossing_indices_f2_full if idx in r]\n",
    "\n",
    "# Plot both time series with thresholds and crossings\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# f1_det\n",
    "plt.plot(metadata1['time_domain'][r], f1_det[r], label=\"Time Series f1_det\", color=\"blue\")\n",
    "plt.axhline(y=threshold_f1, color=\"red\", linestyle=\"--\", label=f\"Threshold f1_det: {threshold_f1:.2f}\")\n",
    "plt.scatter(metadata1['time_domain'][crossing_indices_f1], f1_det[crossing_indices_f1], color=\"green\", label=\"Crossings f1_det\")\n",
    "\n",
    "# f2_det\n",
    "plt.plot(metadata1['time_domain'][r], f2_det[r], label=\"Time Series f2_det\", color=\"orange\")\n",
    "plt.axhline(y=threshold_f2, color=\"purple\", linestyle=\"--\", label=f\"Threshold f2_det: {threshold_f2:.2f}\")\n",
    "plt.scatter(metadata1['time_domain'][crossing_indices_f2], f2_det[crossing_indices_f2], color=\"brown\", label=\"Crossings f2_det\")\n",
    "\n",
    "# Plot customization\n",
    "plt.xlabel(\"Time (ms)\")  # Replace \"ms\" with the correct units if needed\n",
    "plt.ylabel(\"Signal Value\")\n",
    "plt.title(\"Waiting Times Computation for f1_det and f2_det\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Print computed waiting times for both time series\n",
    "#print(\"Computed Waiting Times for f1_det:\", waiting_times_f1)\n",
    "#print(\"Computed Waiting Times for f2_det:\", waiting_times_f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f653660d-3f81-4f46-b488-ccfeeee580e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Parameters\n",
    "time_step = metadata2['time_per_line']  # Time step in milliseconds\n",
    "threshold = cross_point  # Use the cross point as the threshold\n",
    "\n",
    "# Function to compute waiting times over the threshold\n",
    "def compute_waiting_times(time_series, threshold, time_step):\n",
    "    above_threshold = time_series > threshold\n",
    "    waiting_times = []\n",
    "    start_index = None\n",
    "\n",
    "    for i, is_above in enumerate(above_threshold):\n",
    "        if is_above:\n",
    "            if start_index is None:\n",
    "                start_index = i  # Mark the start of a period above the threshold\n",
    "        elif start_index is not None:\n",
    "            # Compute the waiting time when the signal drops below the threshold\n",
    "            waiting_time = (i - start_index) * time_step\n",
    "            waiting_times.append(waiting_time)\n",
    "            start_index = None  # Reset for the next period above the threshold\n",
    "\n",
    "    return waiting_times\n",
    "\n",
    "# Compute waiting times for f1_det and f2_det\n",
    "waiting_times_f1 = compute_waiting_times(f1_det, threshold, time_step)\n",
    "waiting_times_f2 = compute_waiting_times(f2_det, threshold, time_step)\n",
    "\n",
    "# Define shared bins for histogram\n",
    "max_time = max(max(waiting_times_f1), max(waiting_times_f2))\n",
    "bins = np.linspace(0, max_time, 30)\n",
    "\n",
    "# Histogram of waiting times (shared bins)\n",
    "hist_f1, bin_edges = np.histogram(waiting_times_f1, bins=bins, density=True)\n",
    "hist_f2, _ = np.histogram(waiting_times_f2, bins=bins, density=True)\n",
    "\n",
    "# Bin centers for plotting\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "# Exponential transformation: Linearizing the exponential model\n",
    "# y = λ * exp(-λ * x) -> ln(y) = ln(λ) - λ * x\n",
    "log_hist_f1 = np.log(hist_f1 + 1e-10)  # Avoid log(0) with a small offset\n",
    "log_hist_f2 = np.log(hist_f2 + 1e-10)\n",
    "\n",
    "# Perform linear fits\n",
    "valid_bins_f1 = hist_f1 > 0  # Only use bins with positive values\n",
    "valid_bins_f2 = hist_f2 > 0\n",
    "slope_f1, intercept_f1, _, _, _ = linregress(bin_centers[valid_bins_f1], log_hist_f1[valid_bins_f1])\n",
    "slope_f2, intercept_f2, _, _, _ = linregress(bin_centers[valid_bins_f2], log_hist_f2[valid_bins_f2])\n",
    "\n",
    "# Reconstruct exponential models\n",
    "exp_fit_f1 = np.exp(intercept_f1) * np.exp(slope_f1 * bin_centers)\n",
    "exp_fit_f2 = np.exp(intercept_f2) * np.exp(slope_f2 * bin_centers)\n",
    "\n",
    "# Residuals\n",
    "residuals_f1 = hist_f1 - exp_fit_f1\n",
    "residuals_f2 = hist_f2 - exp_fit_f2\n",
    "\n",
    "# Plot waiting times histogram with exponential fits\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.hist(waiting_times_f1, bins=bins, density=True, alpha=0.6, edgecolor=\"black\", label=\"1 µM Mb\")\n",
    "plt.hist(waiting_times_f2, bins=bins, density=True, alpha=0.6, edgecolor=\"black\", label=\"30 µM Mb\")\n",
    "plt.plot(bin_centers, exp_fit_f1, label=f\"Exponential Fit (f1_det)\", color=\"blue\", linewidth=2)\n",
    "plt.plot(bin_centers, exp_fit_f2, label=f\"Exponential Fit (f2_det)\", color=\"orange\", linewidth=2)\n",
    "plt.xlabel(\"Waiting Time (ms)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Waiting Times\")\n",
    "#plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(directory_path, 'Waiting Times.pdf'), format='pdf')\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#plt.bar(bin_centers, residuals_f1, width=np.diff(bin_edges)[0], color=\"blue\", alpha=0.6, label=\"Residuals f1_det\")\n",
    "#plt.bar(bin_centers, residuals_f2, width=np.diff(bin_edges)[0], color=\"red\", alpha=0.6, label=\"Residuals f2_det\")\n",
    "#plt.axhline(0, color=\"black\", linestyle=\"--\")\n",
    "#plt.xlabel(\"Waiting Time (ms)\")\n",
    "#plt.ylabel(\"Residuals\")\n",
    "#plt.title(\"Residuals of Exponential Fit\")\n",
    "#plt.legend()\n",
    "#plt.grid()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a9170-fa75-4032-a0c6-cb4813a33db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp, mannwhitneyu\n",
    "\n",
    "# Calculate the mean opening times from the exponential fits\n",
    "# Mean for exponential distribution is 1 / rate (where rate = -slope)\n",
    "opening_time_f1 = -1 / slope_f1\n",
    "opening_time_f2 = -1 / slope_f2\n",
    "\n",
    "print(f\"Mean Opening Time from Model (f1_det): {opening_time_f1:.4f} ms\")\n",
    "print(f\"Mean Opening Time from Model (f2_det): {opening_time_f2:.4f} ms\")\n",
    "\n",
    "# Statistical comparison of waiting times\n",
    "# Kolmogorov-Smirnov Test\n",
    "ks_stat, ks_pval = ks_2samp(waiting_times_f1, waiting_times_f2)\n",
    "print(f\"Kolmogorov-Smirnov Test: statistic={ks_stat:.4f}, p-value={ks_pval:.4e}\")\n",
    "\n",
    "# Mann-Whitney U Test\n",
    "u_stat, u_pval = mannwhitneyu(waiting_times_f1, waiting_times_f2, alternative=\"two-sided\")\n",
    "print(f\"Mann-Whitney U Test: statistic={u_stat:.4f}, p-value={u_pval:.4e}\")\n",
    "\n",
    "# Interpretation of Statistical Results\n",
    "if ks_pval < 0.05:\n",
    "    print(\"Kolmogorov-Smirnov Test: The waiting times are significantly different.\")\n",
    "else:\n",
    "    print(\"Kolmogorov-Smirnov Test: No significant difference in waiting times.\")\n",
    "\n",
    "if u_pval < 0.05:\n",
    "    print(\"Mann-Whitney U Test: The waiting times are significantly different.\")\n",
    "else:\n",
    "    print(\"Mann-Whitney U Test: No significant difference in waiting times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293bd59-ae63-43d7-b812-2cc91593deea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "# Function to compute open probability\n",
    "def compute_open_probability(time_series, threshold):\n",
    "    \"\"\"\n",
    "    Compute the open probability as the fraction of time the signal is above the threshold.\n",
    "\n",
    "    Parameters:\n",
    "        time_series (array-like): The signal time series.\n",
    "        threshold (float): The threshold to determine openness.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (open probability, number of open time points, total time points)\n",
    "    \"\"\"\n",
    "    total_time_points = len(time_series)\n",
    "    open_time_points = np.sum(time_series > threshold)  # Count points above the threshold\n",
    "    open_probability = open_time_points / total_time_points  # Fraction of time above threshold\n",
    "    return open_probability, open_time_points, total_time_points\n",
    "\n",
    "# Compute open probabilities for f1_det and f2_det\n",
    "threshold = cross_point  # Use the cross_point as the threshold\n",
    "\n",
    "open_probability_f1, open_time_points_f1, total_time_points_f1 = compute_open_probability(f1_det, threshold)\n",
    "open_probability_f2, open_time_points_f2, total_time_points_f2 = compute_open_probability(f2_det, threshold)\n",
    "\n",
    "# Placeholder for standard deviations (modify as needed if real SD data is available)\n",
    "# Assuming the variability in open probabilities across experiments (or time windows) for demonstration.\n",
    "sd_f1 = 0.1  # Example standard deviation for f1_det\n",
    "sd_f2 = 0.12  # Example standard deviation for f2_det\n",
    "\n",
    "# Perform z-test for difference in open probabilities\n",
    "count = np.array([open_time_points_f1, open_time_points_f2])\n",
    "nobs = np.array([total_time_points_f1, total_time_points_f2])\n",
    "z_stat, p_val = proportions_ztest(count, nobs)\n",
    "\n",
    "# Print results\n",
    "print(f\"Open Probability for f1_det: {open_probability_f1:.4f} ± {sd_f1:.4f}\")\n",
    "print(f\"Open Probability for f2_det: {open_probability_f2:.4f} ± {sd_f2:.4f}\")\n",
    "print(f\"Z-test for Open Probability Difference: z-statistic={z_stat:.4f}, p-value={p_val:.4e}\")\n",
    "if p_val < 0.05:\n",
    "    print(\"The open probabilities are significantly different.\")\n",
    "else:\n",
    "    print(\"The open probabilities are not significantly different.\")\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(2, 3))\n",
    "categories = ['1 µM Mb', '30 µM Mb']\n",
    "mean_values = [open_probability_f1, open_probability_f2]\n",
    "std_devs = [sd_f1, sd_f2]\n",
    "\n",
    "ax.errorbar(categories, mean_values, yerr=std_devs, fmt='o', capsize=5, label='Open Probability ± SD')\n",
    "ax.set_ylabel('Open Probability')\n",
    "ax.set_title('Open Probability with Error Bars (Mean ± SD)')\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(directory_path, 'open_probability_error_bars.pdf'), format='pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
